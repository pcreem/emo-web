import gradio as gr
import whisper
import torch
import numpy as np
from feature import (
    AudioTextEmotionModel,
    extract_audio_features,
    extract_text_features
)

# è¨­å®šè¨­å‚™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è¼‰å…¥æ¨¡å‹
emotion_model = AudioTextEmotionModel(audio_input_dim=180, text_input_dim=768, hidden_dim=128, output_dim=3)
emotion_model.load_state_dict(torch.load("model_weights.pth", map_location=device))
emotion_model.to(device)
emotion_model.eval()

# Whisper æ¨¡å‹
whisper_model = whisper.load_model("base")
EMOTION_LABELS = {0: 'æ­£é¢', 1: 'ä¸­æ€§', 2: 'è² é¢'}

# æƒ…ç·’é æ¸¬ä¸»å‡½å¼ï¼ˆæ”¯æ´èªéŸ³ / æ–‡å­— / é›™æ¨¡ï¼‰
def analyze_input(audio, text_input):
    audio_feat = None
    text_feat = None
    result_text = ""
    
    # è‹¥æœ‰èªéŸ³è¼¸å…¥
    if audio:
        result = whisper_model.transcribe(audio, language="zh")
        transcribed_text = result["text"]
        result_text += f"ğŸ§ èªéŸ³è½‰æ–‡å­—ï¼šã€Œ{transcribed_text}ã€\n"
        audio_feat = extract_audio_features(audio)
    else:
        transcribed_text = None

    # è‹¥æœ‰æ–‡å­—è¼¸å…¥ï¼ˆç”¨æˆ¶è¼¸å…¥æˆ–èªéŸ³è½‰å‡ºï¼‰
    text = text_input or transcribed_text
    if text:
        text_feat = extract_text_features(text)
        result_text += f"âœï¸ æ–‡å­—å…§å®¹ï¼šã€Œ{text}ã€\n"
    
    if audio_feat is None and text_feat is None:
        return "è«‹æä¾›èªéŸ³æˆ–æ–‡å­—è¼¸å…¥é€²è¡Œæƒ…ç·’è¾¨è­˜ã€‚"

    # è£½ä½œ tensor è¼¸å…¥
    audio_tensor = (
        torch.tensor(audio_feat, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)
        if audio_feat is not None else
        torch.zeros(1, 1, 180).to(device)
    )
    text_tensor = (
        torch.tensor(text_feat, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)
        if text_feat is not None else
        torch.zeros(1, 1, 768).to(device)
    )

    with torch.no_grad():
        output = emotion_model(audio_tensor, text_tensor)
        pred = torch.argmax(output, dim=1).item()

    result_text += f"ğŸ“Š é æ¸¬æƒ…ç·’ï¼š{EMOTION_LABELS[pred]}"
    return result_text

# Gradio Chat UI
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ§ ä¸­æ–‡èªéŸ³æƒ…ç·’è¾¨è­˜èŠå¤©æ©Ÿå™¨äºº\næ”¯æ´èªéŸ³è¼¸å…¥ã€æ–‡å­—è¼¸å…¥ï¼Œæˆ–å…©è€…çµåˆåˆ†æ")

    chatbot = gr.Chatbot()
    with gr.Row():
        audio_input = gr.Audio(sources=["microphone", "upload"], type="filepath", label="èªéŸ³")
        text_input = gr.Textbox(lines=2, placeholder="è¼¸å…¥æ–‡å­—å…§å®¹...", label="æ–‡å­—")
    send_btn = gr.Button("é€å‡ºåˆ†æ")

    def chat_handler(audio, text, history):
        response = analyze_input(audio, text)
        history = history or []
        history.append(("ğŸ‘¤", response))
        return history, None, ""

    send_btn.click(fn=chat_handler,
                   inputs=[audio_input, text_input, chatbot],
                   outputs=[chatbot, audio_input, text_input])

demo.launch(share=True)
