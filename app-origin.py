import gradio as gr
import whisper
from feature import AudioTextEmotionModel, extract_audio_features
import torch

# è¨­å®šè¨­å‚™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è¼‰å…¥æƒ…ç·’è¾¨è­˜æ¨¡å‹
emotion_model = AudioTextEmotionModel(audio_input_dim=180, text_input_dim=768, hidden_dim=128, output_dim=3)
emotion_model.load_state_dict(torch.load("model_weights.pth", map_location=device))
emotion_model.to(device)
emotion_model.eval()

# è¼‰å…¥ Whisper æ¨¡å‹é€²è¡ŒèªéŸ³è½‰æ–‡å­—
whisper_model = whisper.load_model("base")
EMOTION_LABELS = {0: 'æ­£é¢', 1: 'ä¸­æ€§', 2: 'è² é¢'}

def predict_emotion(audio_path):
    result = whisper_model.transcribe(audio_path, language="zh")
    text = result["text"]
    audio_feat = extract_audio_features(audio_path)
    audio_tensor = torch.tensor(audio_feat, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)

    with torch.no_grad():
        output = emotion_model(audio_tensor, torch.zeros(1, 1, 768).to(device))  # dummy text input
        pred = torch.argmax(output, dim=1).item()

    return f"èªéŸ³è½‰æ–‡å­—çµæœï¼š{text}\né æ¸¬æƒ…ç·’ï¼š{EMOTION_LABELS[pred]}"

def create_interface():
    with gr.Blocks() as demo:
        gr.Markdown("### ğŸ§ ä¸­æ–‡èªéŸ³æƒ…ç·’è¾¨è­˜ï¼ˆEATDï¼‰\nèªªä¸€æ®µè©±ï¼Œæˆ‘æœƒåˆ¤æ–·ä½ çš„æƒ…ç·’ï¼ˆæ­£é¢ / ä¸­æ€§ / è² é¢ï¼‰")
        audio_input = gr.Audio(sources=["microphone"], type="filepath", label="è«‹éŒ„éŸ³")
        output = gr.Textbox()
        btn = gr.Button("åˆ†æ")
        btn.click(fn=predict_emotion, inputs=audio_input, outputs=output)
    return demo

demo = create_interface()
demo.launch(share=True)

